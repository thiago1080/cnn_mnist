docker pull tensorflow/servingdocker run -p 8501:8501 --name=tf_serving_mnist --mount type=bind,source=$(pwd)/saved_model/my_model,target=/models/my_model -e MODEL_NAME=my_model -t tensorflow/servingimport requests

import numpy as np

# Preparar os dados de teste
(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()
x_test = x_test.reshape((x_test.shape[0], 28, 28, 1)).astype('float32') / 255

# Selecionar uma amostra para inferência
sample = x_test[0:1]

# Fazer a requisição para o servidor TensorFlow Serving
url = 'http://localhost:8501/v1/models/my_model:predict'
data = json.dumps({"instances": sample.tolist()})
headers = {"content-type": "application/json"}
response = requests.post(url, data=data, headers=headers)

# Processar a resposta
predictions = np.array(response.json()['predictions'])
print("Predicted class:", np.argmax(predictions))

#---------------------------------------------
#Quantização


converter = tf.lite.TFLiteConverter.from_saved_model('saved_model/my_model')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

# Salvar o modelo quantizado
with open('model_quantized.tflite', 'wb') as f:
    f.write(tflite_model)



#---------------------------------------------
# Tensor RT

import tensorflow as tf
from tensorflow.python.compiler.tensorrt import trt_convert as trt

# Converter o modelo para TensorRT
params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode='FP16')
converter = trt.TrtGraphConverterV2(input_saved_model_dir='saved_model/my_model', conversion_params=params)
converter.convert()
converter.save('saved_model/my_model_trt')



# Fast api


from fastapi import FastAPI
import tensorflow as tf
import numpy as np

app = FastAPI()

# Carregar o modelo
model = tf.keras.models.load_model('saved_model/my_model')

@app.post("/predict/")
async def predict(data: List[List[List[float]]]):
    input_data = np.array(data).reshape((1, 28, 28, 1))
    predictions = model.predict(input_data)
    return {"predictions": predictions.tolist()}

# Rodar o servidor FastAPI
# Execute este comando no terminal: uvicorn script_name:app --reload